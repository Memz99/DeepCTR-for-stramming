{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder, MinMaxScaler\n",
    "\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "from deepctr_torch.models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "demo先直接pandas读。\n",
    "数据量大时，应该会分成多个文件，这时\"统计词表\"，sparse、dense特征以及特征域的维度都应以config形式传入；\n",
    "然后直接根据词表构建embedding就行；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 统计历史词表\n",
    "这部分后面应提出主程序之外，磁盘上事先写好 word2idx, idx2word 两个表，主程序直接读取即可。  \n",
    "这里使用5月15日的raw表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "feat2idx =  {'user_id': (0, 1),\n",
    "             'keyword': (1, 2),\n",
    "             'sequence_keyword': (2, 3),\n",
    "             'search_source': (3, 4),\n",
    "             'session_id': (4, 5),\n",
    "             'item_id': (5, 6),\n",
    "             'show_cnt': (6, 7),\n",
    "             'click_cnt': (7, 8),\n",
    "             'play_cnt': (8, 9),\n",
    "             'like_cnt': (9, 10),\n",
    "             'follow_cnt': (10, 11),\n",
    "             'long_view_cnt': (11, 12),\n",
    "             'short_view_cnt': (12, 13),\n",
    "             'first_click': (13, 14),\n",
    "             'last_click': (14, 15),\n",
    "             'first_view': (15, 16),\n",
    "             'last_view': (16, 17),\n",
    "             'skip': (17, 18),\n",
    "             'exam': (18, 19),\n",
    "             'play_duration': (19, 20),\n",
    "             'slide_show': (20, 21),\n",
    "             'slide_click': (21, 22),\n",
    "             'pos': (22, 23),\n",
    "             'atlas_view_cnt': (23, 24),\n",
    "             'download_cnt': (24, 25),\n",
    "             'feed_model': (25, 26),\n",
    "             'p_date': (26, 27),\n",
    "             'product': (27, 28)}\n",
    "\n",
    "\n",
    "sparse_features = ['user_id', 'keyword', 'sequence_keyword', 'search_source', 'session_id', 'item_id', \n",
    "                   'first_click', 'last_click', 'first_view', 'last_view',\n",
    "                   'pos', 'feed_model', 'p_date', 'product']\n",
    "\n",
    "dense_features = ['show_cnt', 'click_cnt', 'play_cnt', 'like_cnt', 'follow_cnt', 'long_view_cnt', \n",
    "                  'short_view_cnt', 'slide_show', 'slide_click', 'atlas_view_cnt']\n",
    "\n",
    "data = pd.read_csv(\"data/raw/20210516\", sep=\"\\t\", dtype={feat: str for feat in sparse_features}) \n",
    "## 缺失值处理\n",
    "for feat in sparse_features + dense_features:\n",
    "    if feat in sparse_features:\n",
    "        data[feat] = data[feat].fillna(\"\")\n",
    "    else:\n",
    "        data[feat] = data[feat].fillna(0)\n",
    "\n",
    "## 离散特征编码\n",
    "sparse_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "data[sparse_features] = sparse_encoder.fit_transform(data[sparse_features])\n",
    "\n",
    "sparse_feature_info = {}\n",
    "for fname, word_list in zip(sparse_features, sparse_encoder.categories_):\n",
    "    vocab = {word: i for i, word in enumerate(np.concatenate((word_list, [\"__UNK__\"])))}\n",
    "    sparse_feature_info[fname] = (feat2idx[fname], vocab)\n",
    "dense_feature_info = {}\n",
    "for fname in dense_features:\n",
    "    dense_feature_info[fname] = feat2idx[fname]\n",
    "    \n",
    "# encoder_dict = {fname: {word: i\n",
    "#                           for i, word in enumerate(\n",
    "#                               np.concatenate((word_list, [\"__UNK__\"])))} # vocab中添加 __UNK__ token\n",
    "#                  for fname, word_list in zip(sparse_features, sparse_encoder.categories_)}\n",
    "# idx_encoder = [[feat2idx[name], vocab] for name, vocab in encoder_dict.items()]\n",
    "\n",
    "# ## 连续特征归一化\n",
    "# dense_encoder = MinMaxScaler(feature_range=(0,1))\n",
    "# data[dense_features] = dense_encoder.fit_transform(data[dense_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 制作 iterable dataset\n",
    "输入: sparse_feature_info = {feature_name: (index, vocab), }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
    "\n",
    "def raw_iterator(files, sparse_feature_info):\n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            feat_names = f.readline()\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            row = np.array(line.strip().split('\\t'))\n",
    "            # encoder\n",
    "            # row[sparse_idx] = [vocab[row[i]] if row[i] in vocab else vocab[\"__UNK__\"]\n",
    "            #                    for i, vocab in idx_encoder]\n",
    "            for feat, (index, vocab) in sparse_feature_info.items():\n",
    "                idxs = list(range(*index))\n",
    "                row[idxs] = [vocab[row[i]] if row[i] in vocab else vocab[\"__UNK__\"]\n",
    "                             for i in idxs]\n",
    "            row = row.astype(float)\n",
    "            yield row\n",
    "\n",
    "\n",
    "class Raw_Dataset(IterableDataset):\n",
    "    def __init__(self, iterator, label_idx):\n",
    "        self.iterator = iterator\n",
    "        self.label_idx = label_idx\n",
    "    def __iter__(self):\n",
    "        for x in self.iterator:\n",
    "            y = np.array([1]) if x[self.label_idx] > 0 else np.array([0])\n",
    "            yield {\"features\": x, \"label\": y}\n",
    "\n",
    "iterator = raw_iterator([\"data/raw/20210516\"], sparse_feature_info)\n",
    "ds = Raw_Dataset(iterator, feat2idx[\"click_cnt\"][0])\n",
    "loader = DataLoader(ds, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 模型定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 feature_column定义\n",
    "会用于\n",
    "1. sparse feature 的 embedding 表定义\n",
    "2. 模型 forward 的最开始时，从输入的 batch 中, 根据 feature_column 的下标分别抽出 wide_feature 和 deep_feature 对应的 tensor 列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict, namedtuple, defaultdict\n",
    "\n",
    "DEFAULT_GROUP_NAME = \"default_group\"\n",
    "\n",
    "class SparseFeat(namedtuple('SparseFeat',\n",
    "                            ['name', 'index', 'vocabulary_size', 'embedding_dim', 'dimension','sparse_embedding', 'dtype', 'embedding_name',\n",
    "                             'group_name'])):\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, name, index, vocabulary_size, embedding_dim=4, sparse_embedding=False, dtype=\"int32\",\n",
    "                embedding_name=None, group_name=DEFAULT_GROUP_NAME):\n",
    "        if embedding_name is None:\n",
    "            embedding_name = name\n",
    "        if embedding_dim == \"auto\":\n",
    "            embedding_dim = 6 * int(pow(vocabulary_size, 0.25))\n",
    "        return super(SparseFeat, cls).__new__(cls, name, index, vocabulary_size, embedding_dim, embedding_dim,\n",
    "                                              sparse_embedding, dtype, embedding_name, group_name)\n",
    "\n",
    "\n",
    "class VarLenSparseFeat(namedtuple('VarLenSparseFeat',\n",
    "                                  ['sparsefeat', 'maxlen', 'index', 'combiner', 'length_name'])):\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, sparsefeat, maxlen, index, combiner=\"mean\", length_name=None):\n",
    "        return super(VarLenSparseFeat, cls).__new__(cls, sparsefeat, maxlen, index, combiner, length_name)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.sparsefeat.name\n",
    "\n",
    "    @property\n",
    "    def vocabulary_size(self):\n",
    "        return self.sparsefeat.vocabulary_size\n",
    "    \n",
    "    @property\n",
    "    def sparse_embedding(self):\n",
    "        return self.sparsefeat.sparse_embedding\n",
    "\n",
    "    @property\n",
    "    def embedding_dim(self):\n",
    "        return self.sparsefeat.embedding_dim\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.sparsefeat.dtype\n",
    "\n",
    "    @property\n",
    "    def embedding_name(self):\n",
    "        return self.sparsefeat.embedding_name\n",
    "\n",
    "class DenseFeat(namedtuple('DenseFeat', ['name', 'index', 'dimension', 'dtype'])):\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, name, index, dimension=1, dtype=\"float32\"):\n",
    "        return super(DenseFeat, cls).__new__(cls, name, index, dimension, dtype)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.name.__hash__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from sklearn.metrics import *\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_embedding_matrix(feature_columns, init_std=0.0001, linear=False, sparse=False, device='cpu'):\n",
    "    # Return nn.ModuleDict: for sparse features, {embedding_name: nn.Embedding}\n",
    "    # for varlen sparse features, {embedding_name: nn.EmbeddingBag}\n",
    "    sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "    varlen_sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "    embedding_dict = nn.ModuleDict(\n",
    "        {feat.embedding_name: nn.Embedding(feat.vocabulary_size, feat.embedding_dim if not linear else 1, sparse=sparse)\n",
    "         for feat in\n",
    "         sparse_feature_columns + varlen_sparse_feature_columns}\n",
    "    )\n",
    "\n",
    "    # for feat in varlen_sparse_feature_columns:\n",
    "    #     embedding_dict[feat.embedding_name] = nn.EmbeddingBag(\n",
    "    #         feat.dimension, embedding_size, sparse=sparse, mode=feat.combiner)\n",
    "\n",
    "    for tensor in embedding_dict.values():\n",
    "        nn.init.normal_(tensor.weight, mean=0, std=init_std)\n",
    "\n",
    "    return embedding_dict.to(device)\n",
    "\n",
    "def activation_layer(act_name):\n",
    "    if isinstance(act_name, str):\n",
    "        if act_name.lower() == 'sigmoid':\n",
    "            act_layer = nn.Sigmoid()\n",
    "        elif act_name.lower() == 'linear':\n",
    "            act_layer = Identity()\n",
    "        elif act_name.lower() == 'relu':\n",
    "            act_layer = nn.ReLU(inplace=True)\n",
    "        elif act_name.lower() == 'prelu':\n",
    "            act_layer = nn.PReLU()\n",
    "    elif issubclass(act_name, nn.Module):\n",
    "        act_layer = act_name()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return act_layer\n",
    "\n",
    "class DNN(nn.Module):\n",
    "\n",
    "    def __init__(self, inputs_dim, hidden_units, activation='relu', dropout_rate=0, init_std=0.0001):\n",
    "        super(DNN, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        hidden_units = [inputs_dim] + list(hidden_units)\n",
    "\n",
    "        self.linears = nn.ModuleList(\n",
    "            [nn.Linear(hidden_units[i], hidden_units[i + 1]) for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "        self.activation_layers = nn.ModuleList(\n",
    "            [activation_layer(activation) for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "        for name, tensor in self.linears.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(tensor, mean=0, std=init_std)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        for i in range(len(self.linears)):\n",
    "            x = self.linears[i](x)\n",
    "            x = self.activation_layers[i](x)\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, sparse_feature_columns, dense_feature_columns, \n",
    "                 init_std=0.0001, device='cpu'):\n",
    "        super(Linear, self).__init__()\n",
    "        self.sparse_feature_columns = sparse_feature_columns\n",
    "        self.dense_feature_columns = dense_feature_columns\n",
    "\n",
    "        self.embedding_dict = create_embedding_matrix(sparse_feature_columns)\n",
    "\n",
    "        for tensor in self.embedding_dict.values():\n",
    "            nn.init.normal_(tensor.weight, mean=0, std=init_std)\n",
    "\n",
    "        if len(self.dense_feature_columns) > 0:\n",
    "            self.weight = nn.Parameter(torch.Tensor(sum(fc.dimension for fc in self.dense_feature_columns), 1).to(\n",
    "                device))\n",
    "            torch.nn.init.normal_(self.weight, mean=0, std=init_std)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        dense_value_list = [inputs[:, feat.index[0]:feat.index[1]] \n",
    "                          for feat in self.dense_feature_columns]\n",
    "\n",
    "        sparse_embedding_list = [\n",
    "            self.embedding_dict[feat.name]( # 取出 embedding 表\n",
    "                inputs[:, feat.index[0]:feat.index[1]])\n",
    "            for feat in self.sparse_feature_columns]\n",
    "        \n",
    "#         sparse_embedding_list = [self.embedding_dict[feat.embedding_name](\n",
    "#             X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]].long()) for\n",
    "#             feat in self.sparse_feature_columns]\n",
    "\n",
    "#         dense_value_list = [X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]] for feat in\n",
    "#                             self.dense_feature_columns]\n",
    "\n",
    "        linear_logit = torch.zeros([X.shape[0], 1]).to(sparse_embedding_list[0].device)\n",
    "        if len(sparse_embedding_list) > 0:\n",
    "            sparse_embedding_cat = torch.cat(sparse_embedding_list, dim=-1)\n",
    "            sparse_feat_logit = torch.sum(sparse_embedding_cat, dim=-1, keepdim=False)\n",
    "            linear_logit += sparse_feat_logit\n",
    "        if len(dense_value_list) > 0:\n",
    "            dense_value_logit = torch.cat(\n",
    "                dense_value_list, dim=-1).matmul(self.weight)\n",
    "            linear_logit += dense_value_logit\n",
    "\n",
    "        return linear_logit\n",
    "\n",
    "    \n",
    "class FM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FM, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        fm_input = inputs\n",
    "\n",
    "        square_of_sum = torch.pow(torch.sum(fm_input, dim=1, keepdim=True), 2)\n",
    "        sum_of_square = torch.sum(fm_input * fm_input, dim=1, keepdim=True)\n",
    "        cross_term = square_of_sum - sum_of_square\n",
    "        cross_term = 0.5 * torch.sum(cross_term, dim=2, keepdim=False)\n",
    "\n",
    "        return cross_term\n",
    "\n",
    "\n",
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, sparse_feature_columns, dense_feature_columns,\n",
    "                 dnn_hidden_units=(256, 128), dnn_activation='ReLU', dnn_dropout=0,\n",
    "                 init_std=0.0001, seed=1173, device='cpu'):\n",
    "        super(DeepFM, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.sparse_feature_columns = sparse_feature_columns\n",
    "        self.dense_feature_columns = dense_feature_columns\n",
    "        \n",
    "        self.embedding_dict = create_embedding_matrix(sparse_feature_columns)\n",
    "        # 正则项...\n",
    "        \n",
    "        # DNN\n",
    "        dnn_input_dim = sum(feat.dimension for feat in sparse_feature_columns + dense_feature_columns)\n",
    "        self.dnn = DNN(dnn_input_dim, dnn_hidden_units,\n",
    "               activation=dnn_activation, dropout_rate=dnn_dropout)\n",
    "        \n",
    "        # FM\n",
    "        self.linear = Linear(sparse_feature_columns, dense_feature_columns)\n",
    "        self.fm = FM()\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    def split_tensor(self, inputs):\n",
    "\n",
    "        dense_value_list = [inputs[:, feat.index[0]:feat.index[1]] \n",
    "                          for feat in self.dense_feature_columns]\n",
    "\n",
    "        sparse_embedding_list = [\n",
    "            self.embedding_dict[feat.name]( # 取出 embedding 表\n",
    "                inputs[:, feat.index[0]:feat.index[1]])\n",
    "            for feat in self.sparse_feature_columns]\n",
    "\n",
    "        return sparse_embedding_list, dense_value_list\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        sparse_embedding_list, dense_value_list = self.split_tensor(inputs)\n",
    "        return sparse_embedding_List, dense_value_list\n",
    "#         level1_logit = self.linear_model(X)\n",
    "\n",
    "#         if len(sparse_embedding_list) > 0:\n",
    "#             fm_input = torch.cat(sparse_embedding_list, dim=1)\n",
    "#             logit += self.fm(fm_input)\n",
    "\n",
    "#         dnn_input = combined_dnn_input(\n",
    "#             sparse_embedding_list, dense_value_list)\n",
    "#         dnn_output = self.dnn(dnn_input)\n",
    "#         dnn_logit = self.dnn_linear(dnn_output)\n",
    "#         logit += dnn_logit\n",
    "\n",
    "#         y_pred = self.out(logit)\n",
    "\n",
    "#         return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_feature_columns = [SparseFeat(name, index, len(vocab), 4, False)\n",
    "                          for name, (index, vocab) in sparse_feature_info.items()]\n",
    "dense_feature_columns = [DenseFeat(name, index) for name, index in dense_feature_info.items()]\n",
    "\n",
    "model = DeepFM(sparse_feature_columns, dense_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user_id'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name, (index, vocab) = list(sparse_feature_info.items())[0]\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-12922fcb1b7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-77c4ac408a0b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0msparse_embedding_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_value_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msparse_embedding_List\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_value_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;31m#         level1_logit = self.linear_model(X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-77c4ac408a0b>\u001b[0m in \u001b[0;36msplit_tensor\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         dense_value_list = [inputs[:, feat.index[0]:feat.index[1]] \n\u001b[0;32m--> 166\u001b[0;31m                           for feat in self.dense_feature_columns]\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         sparse_embedding_list = [\n",
      "\u001b[0;32m<ipython-input-31-77c4ac408a0b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         dense_value_list = [inputs[:, feat.index[0]:feat.index[1]] \n\u001b[0;32m--> 166\u001b[0;31m                           for feat in self.dense_feature_columns]\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         sparse_embedding_list = [\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "for inputs in loader:\n",
    "    sparse, dense = model(inputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hemingzhi_py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
